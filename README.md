
# Durable Image Metadata Processing Pipeline

ğŸ“ **Course:** CST8917 â€“ Serverless Computing  
ğŸ§  **Assignment:** Durable Workflow for Image Metadata Processing  
ğŸ“… **Due Date:** July 19, 2025  
ğŸ‘¨â€ğŸ’» **Author:** Tarang Savaj ([tarang-0508](https://github.com/tarang-0508))

---

## ğŸ“Œ Overview

This project implements a **serverless image metadata processing pipeline** using **Azure Durable Functions** in Python. The solution automatically triggers when an image is uploaded to Azure Blob Storage, extracts its metadata (size, dimensions, format), and stores it in an **Azure SQL Database** using output bindings.

---

## ğŸ“‚ Architecture

```text
         +-------------------------+
         |  Blob Storage Upload    |
         | (.jpg/.png/.gif files)  |
         +-----------+-------------+
                     |
                     v
         +-------------------------+
         |   BlobTriggerFunction   |
         | Starts the Orchestrator |
         +-----------+-------------+
                     |
                     v
         +-------------------------+
         |   OrchestratorFunction  |
         | Calls activity functions|
         +-----------+-------------+
                     |
     +---------------+--------------------+
     |                                    |
     v                                    v
+--------------+                  +------------------+
| ExtractMetadata |              |   StoreMetadata   |
| (Reads image &  |              |  (Writes metadata |
| extracts details)|             |   to SQL Table)   |
+----------------+               +------------------+
```

---

## ğŸ› ï¸ Technologies Used

- Azure Durable Functions (Python)
- Azure Blob Storage
- Azure SQL Database
- Azurite for local blob testing
- VS Code + Azure Functions Core Tools
- Python 3.10

---

## ğŸ“¦ Folder Structure

```bash
durable-image-pipeline/
â”‚
â”œâ”€â”€ BlobTriggerFunction/         # Triggers workflow on image upload
â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ function.json
â”‚
â”œâ”€â”€ OrchestratorFunction/        # Coordinates activity functions
â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ function.json
â”‚
â”œâ”€â”€ ExtractMetadata/             # Extracts image metadata
â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ function.json
â”‚
â”œâ”€â”€ StoreMetadata/               # Stores metadata in SQL DB
â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ function.json
â”‚
â”œâ”€â”€ host.json
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ local.settings.json (excluded)
â””â”€â”€ README.md
```

---

## ğŸ§ª How to Run Locally

1. **Create virtual environment**
   ```bash
   python -m venv .venv
   .venv\Scripts\activate
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Start Azurite for local storage**
   ```bash
   npm install -g azurite
   azurite
   ```

4. **Start Azure Functions locally**
   ```bash
   func start
   ```

5. **Upload a test image**  
   Upload `.jpg`, `.png`, or `.gif` image to the `images-input` container using Azure Storage Explorer.

6. **Check logs and SQL database**  
   Confirm that metadata was extracted and stored in your Azure SQL table.

---

## ğŸ§¾ SQL Table Schema

```sql
CREATE TABLE ImageMetadata (
    file_name NVARCHAR(255),
    file_size_kb INT,
    width INT,
    height INT,
    format NVARCHAR(50)
);
```

---

## ğŸ” Sample `local.settings.json` (DO NOT commit this)

```json
{
  "IsEncrypted": false,
  "Values": {
    "AzureWebJobsStorage": "<your-blob-connection-string>",
    "SqlConnectionString": "<your-sql-connection-string>",
    "FUNCTIONS_WORKER_RUNTIME": "python"
  }
}
```

---

## ğŸ“½ï¸ Demo Video

ğŸ”— YouTube Demo Link Here  
_(Replace with your real YouTube demo before submission)_

---

